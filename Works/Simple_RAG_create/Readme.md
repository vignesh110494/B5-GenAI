# ğŸ“„ Simple RAG App â€“ Ask Your Document

A simple Retrieval-Augmented Generation (RAG) application built using:

- **Streamlit** (User Interface)
- **LangChain (LCEL Architecture)**
- **FAISS** (Vector Database)
- **OpenAI Embeddings**
- **ChatOpenAI (gpt-4.1-mini)**

This app allows users to upload a PDF and ask questions grounded strictly in the document content.

---

# ğŸš€ Features

- ğŸ“‚ Upload PDF document  
- âœ‚ Automatic document chunking  
- ğŸ§  Generate OpenAI embeddings  
- ğŸ“Š Store vectors in FAISS  
- ğŸ” Retrieve Top-K relevant chunks (k=4)  
- ğŸ¤– Generate answers using GPT-4.1-mini  
- ğŸ”„ Spinner while generating responses  
- ğŸ–¥ Debug print statements for each stage  

---

# ğŸ— Architecture Flow

```
User Upload PDF
        â†“
PyPDFLoader (Extract Text)
        â†“
RecursiveCharacterTextSplitter
        â†“
OpenAI Embeddings
        â†“
FAISS Vector Store
        â†“
Top-K Retrieval (k=4)
        â†“
Prompt + LLM
        â†“
Display Answer in Streamlit
```

---

# ğŸ§° Tech Stack

| Component | Technology |
|------------|------------|
| UI | Streamlit |
| LLM | gpt-4.1-mini |
| Embeddings | OpenAIEmbeddings |
| Vector DB | FAISS |
| Framework | LangChain (LCEL) |
| Document Loader | PyPDFLoader |

---

# ğŸ“¦ Installation

## 1ï¸âƒ£ Create Project Folder

```bash
mkdir simple_rag_app
cd simple_rag_app
```

---

## 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
python -m venv venv
```

### Activate environment:

**Windows**
```bash
venv\Scripts\activate
```

**Mac/Linux**
```bash
source venv/bin/activate
```

---

## 3ï¸âƒ£ Install Required Packages

```bash
pip install streamlit langchain langchain-core langchain-community langchain-openai faiss-cpu pypdf python-dotenv
```

---

# ğŸ” Environment Configuration

Create a `.env` file in the root directory:

```
OPENAI_API_KEY=your_openai_api_key_here
```

âš  Do not include quotes.

---

# â–¶ï¸ Running the Application

If your file name is:

### app.py
```bash
streamlit run app.py
```

### demo.py
```bash
streamlit run demo.py
```

After running, open your browser at:

```
http://localhost:8501
```

---

# âš™ Configuration Details

## Chunking Settings

```python
chunk_size = 1000
chunk_overlap = 200
```

## Retrieval Settings

```python
search_kwargs = {"k": 4}
```

## LLM Settings

```python
model="gpt-4.1-mini"
temperature=0.2
```

---

# ğŸ” How It Works

1. User uploads a PDF.
2. The document is loaded using `PyPDFLoader`.
3. Text is split into overlapping chunks.
4. Each chunk is converted into embeddings.
5. FAISS stores embeddings in memory.
6. When a user asks a question:
   - Top 4 similar chunks are retrieved.
   - Context + question are sent to the LLM.
   - LLM generates a grounded answer.
7. The answer is displayed in the Streamlit UI.

---

# ğŸ–¥ User Interface Flow

- Upload a PDF file.
- Enter a question.
- Spinner appears while processing.
- Answer is displayed below.

---

# ğŸ” Debug Logging

The app prints the following stages in the terminal:

- File upload success
- Document loading
- Number of pages loaded
- Chunk splitting
- Total chunks created
- Embedding creation
- Vector store initialization
- LLM initialization
- RAG chain creation
- Response generation

---

# ğŸ“ Project Structure

```
simple_rag_app/
â”‚
â”œâ”€â”€ app.py (or demo.py)
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
```

---

# ğŸš€ Future Improvements

- Persistent FAISS index  
- Embedding caching  
- Streaming token output  
- Chat-style UI  
- Source document highlighting  
- Token usage tracking  
- Deployment to Streamlit Cloud  
- Docker containerization  

---

# ğŸ“œ License

This project is for educational purposes.

---

# ğŸ‘¨â€ğŸ’» Author

Built to understand and implement modern Retrieval-Augmented Generation (RAG) using LangChain LCEL architecture.

---

If you'd like, I can also generate:

- âœ… `requirements.txt`
- âœ… Dockerfile
- âœ… Production-ready folder structure
- âœ… Streamlit Cloud deployment guide
- âœ… Optimized version with caching

Just let me know ğŸš€
